{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows the classification of fake papers based on metadata and journal features as well as TF-IDF and BERT embeddings\n",
    "\n",
    "Each cell has a description above it. If there are doubts in any parts of the code please contact at (ahmar.hussain@ovgu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the dataset  (It is in the Github repository named: 'Classification-of-fake-papers-in-biomedicine-with-machine-learning/dataset without full text with masked features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('D:\\\\new_dataset\\\\final_dataset_bigger.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PMID', 'DOI', 'citation_count', 'affiliation_country',\n",
       "       'numer_of_authors', 'open_access', 'orcid_availability',\n",
       "       'hospital_affiliation', 'title_count', 'abstract_count',\n",
       "       'journal_paper', 'title_paper', 'abstract_paper', 'target_variable',\n",
       "       'journal_year', 'Total Citations', 'Journal impact factor',\n",
       "       'JIF without self cites', 'Immediacy Index', 'Citable items',\n",
       "       '% of articles in Citable items', 'Average JIF Percentile',\n",
       "       'Total Articles', 'Total Reviews', 'Cited Half-Life',\n",
       "       'Citing Half-Life', 'Eigenfactor score', 'Normalized Eigenfactor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>DOI</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>affiliation_country</th>\n",
       "      <th>numer_of_authors</th>\n",
       "      <th>open_access</th>\n",
       "      <th>orcid_availability</th>\n",
       "      <th>hospital_affiliation</th>\n",
       "      <th>title_count</th>\n",
       "      <th>abstract_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Immediacy Index</th>\n",
       "      <th>Citable items</th>\n",
       "      <th>% of articles in Citable items</th>\n",
       "      <th>Average JIF Percentile</th>\n",
       "      <th>Total Articles</th>\n",
       "      <th>Total Reviews</th>\n",
       "      <th>Cited Half-Life</th>\n",
       "      <th>Citing Half-Life</th>\n",
       "      <th>Eigenfactor score</th>\n",
       "      <th>Normalized Eigenfactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30867252</td>\n",
       "      <td>10.1042/BSR20190043</td>\n",
       "      <td>45</td>\n",
       "      <td>China</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>122</td>\n",
       "      <td>67.21</td>\n",
       "      <td>58.9</td>\n",
       "      <td>82</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.01450</td>\n",
       "      <td>3.16874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31002124</td>\n",
       "      <td>10.26355/eurrev_201904_17547</td>\n",
       "      <td>7</td>\n",
       "      <td>China</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31858537</td>\n",
       "      <td>10.26355/eurrev_201912_19768</td>\n",
       "      <td>6</td>\n",
       "      <td>China</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31298331</td>\n",
       "      <td>10.26355/eurrev_201907_18318</td>\n",
       "      <td>18</td>\n",
       "      <td>China</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "      <td>237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31114982</td>\n",
       "      <td>10.26355/eurrev_201905_17780</td>\n",
       "      <td>4</td>\n",
       "      <td>China</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31858552</td>\n",
       "      <td>10.26355/eurrev_201912_19787</td>\n",
       "      <td>10</td>\n",
       "      <td>China</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31210306</td>\n",
       "      <td>10.26355/eurrev_201906_18059</td>\n",
       "      <td>10</td>\n",
       "      <td>China</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31378884</td>\n",
       "      <td>10.26355/eurrev_201908_18528</td>\n",
       "      <td>3</td>\n",
       "      <td>China</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31298312</td>\n",
       "      <td>10.26355/eurrev_201907_18294</td>\n",
       "      <td>23</td>\n",
       "      <td>China</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "      <td>238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31298333</td>\n",
       "      <td>10.26355/eurrev_201907_18320</td>\n",
       "      <td>2</td>\n",
       "      <td>China</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>86.94</td>\n",
       "      <td>50.5</td>\n",
       "      <td>972</td>\n",
       "      <td>146.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.02441</td>\n",
       "      <td>5.31418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                           DOI  citation_count affiliation_country  \\\n",
       "0  30867252           10.1042/BSR20190043              45               China   \n",
       "1  31002124  10.26355/eurrev_201904_17547               7               China   \n",
       "2  31858537  10.26355/eurrev_201912_19768               6               China   \n",
       "3  31298331  10.26355/eurrev_201907_18318              18               China   \n",
       "4  31114982  10.26355/eurrev_201905_17780               4               China   \n",
       "5  31858552  10.26355/eurrev_201912_19787              10               China   \n",
       "6  31210306  10.26355/eurrev_201906_18059              10               China   \n",
       "7  31378884  10.26355/eurrev_201908_18528               3               China   \n",
       "8  31298312  10.26355/eurrev_201907_18294              23               China   \n",
       "9  31298333  10.26355/eurrev_201907_18320               2               China   \n",
       "\n",
       "   numer_of_authors  open_access  orcid_availability  hospital_affiliation  \\\n",
       "0                 4         True               False                 False   \n",
       "1                 3        False               False                  True   \n",
       "2                 9        False               False                  True   \n",
       "3                 6        False               False                  True   \n",
       "4                 6        False               False                  True   \n",
       "5                 6        False               False                  True   \n",
       "6                 8        False               False                 False   \n",
       "7                 2        False               False                  True   \n",
       "8                 5        False               False                  True   \n",
       "9                 6        False               False                  True   \n",
       "\n",
       "   title_count  abstract_count  ... Immediacy Index Citable items  \\\n",
       "0           12             246  ...             0.8           122   \n",
       "1           12             168  ...             0.4          1118   \n",
       "2           15             215  ...             0.4          1118   \n",
       "3           13             237  ...             0.4          1118   \n",
       "4           15             203  ...             0.4          1118   \n",
       "5           14             216  ...             0.4          1118   \n",
       "6           15             182  ...             0.4          1118   \n",
       "7           15             211  ...             0.4          1118   \n",
       "8           13             238  ...             0.4          1118   \n",
       "9           15             205  ...             0.4          1118   \n",
       "\n",
       "  % of articles in Citable items  Average JIF Percentile  Total Articles  \\\n",
       "0                          67.21                    58.9              82   \n",
       "1                          86.94                    50.5             972   \n",
       "2                          86.94                    50.5             972   \n",
       "3                          86.94                    50.5             972   \n",
       "4                          86.94                    50.5             972   \n",
       "5                          86.94                    50.5             972   \n",
       "6                          86.94                    50.5             972   \n",
       "7                          86.94                    50.5             972   \n",
       "8                          86.94                    50.5             972   \n",
       "9                          86.94                    50.5             972   \n",
       "\n",
       "   Total Reviews  Cited Half-Life  Citing Half-Life  Eigenfactor score  \\\n",
       "0           40.0              4.5               8.4            0.01450   \n",
       "1          146.0              3.8               6.6            0.02441   \n",
       "2          146.0              3.8               6.6            0.02441   \n",
       "3          146.0              3.8               6.6            0.02441   \n",
       "4          146.0              3.8               6.6            0.02441   \n",
       "5          146.0              3.8               6.6            0.02441   \n",
       "6          146.0              3.8               6.6            0.02441   \n",
       "7          146.0              3.8               6.6            0.02441   \n",
       "8          146.0              3.8               6.6            0.02441   \n",
       "9          146.0              3.8               6.6            0.02441   \n",
       "\n",
       "   Normalized Eigenfactor  \n",
       "0                 3.16874  \n",
       "1                 5.31418  \n",
       "2                 5.31418  \n",
       "3                 5.31418  \n",
       "4                 5.31418  \n",
       "5                 5.31418  \n",
       "6                 5.31418  \n",
       "7                 5.31418  \n",
       "8                 5.31418  \n",
       "9                 5.31418  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting boolean entries (True and False) to integers (1 and 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['orcid_availability'] = data['orcid_availability'].astype(int)\n",
    "data['open_access'] = data['open_access'].astype(int)\n",
    "data['hospital_affiliation'] = data['hospital_affiliation'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding affiliation_country and journal_paper column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiliation_country\n",
    "data = pd.get_dummies(data, columns = ['affiliation_country'])\n",
    "one_hot_encoded_country_columns = [col for col in data.columns if 'affiliation_' in col]    #converting boolean values to integer\n",
    "data[one_hot_encoded_country_columns] = data[one_hot_encoded_country_columns].astype(int)\n",
    "\n",
    "#journal_paper\n",
    "data = pd.get_dummies(data, columns = ['journal_paper'])\n",
    "one_hot_encoded_country_columns = [col for col in data.columns if 'journal_paper' in col]    #converting boolean values to integer\n",
    "data[one_hot_encoded_country_columns] = data[one_hot_encoded_country_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we preprocess the abstract + title of the papers by lemmatizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def lemmatize_text(abstract_and_title):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    abstract_and_title = abstract_and_title.lower()\n",
    "    tokens = word_tokenize(abstract_and_title)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "data['abstract+title'] = (data['abstract_paper'] + ' ' + data['title_paper']).apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we apply different methods to handle the abstract+title columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we used TF-IDF (only run this cell if you want to use TF-IDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['background', 'methods', 'results', 'conclusions', 'conclusion', 'method'])  #we remove these words as well to avoid bias of any abstract structure for specific journals\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "tfidf_vectors = vectorizer.fit_transform(data['abstract+title'])\n",
    "tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we use word2vec (only run this cell if you want to use word2vec word embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize stopwords and add custom stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['background', 'methods', 'results', 'conclusions', 'conclusion', 'method'])\n",
    "\n",
    "data['abstract_tokenized'] = data['abstract_lemmatized'].apply(lambda x: [word for word in word_tokenize(x.lower()) if word not in stopwords])\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=data['abstract_tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def create_document_vector(document):\n",
    "    document = [word for word in document if word in word2vec_model.wv.key_to_index]\n",
    "\n",
    "    if len(document) == 0:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    return np.mean(word2vec_model.wv[document], axis = 0)\n",
    "\n",
    "data['word2vec_vector'] = data['abstract_tokenized'].apply(create_document_vector)\n",
    "\n",
    "word2vec_df = pd.DataFrame(data['word2vec_vector'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we use BERT embeddings (only run this cell if you want to use BERT embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the models used are the following, please change the LLM_model_name variable for different models\n",
    "\n",
    "dmis-lab/biobert-v1.1  \n",
    "emilyalsentzer/Bio_ClinicalBERT  \n",
    "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext  \n",
    "allenai/scibert_scivocab_uncased  \n",
    "bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16  \n",
    "emilyalsentzer/Bio_ClinicalBERT  \n",
    "microsoft/biogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LLM_model_name = 'dmis-lab/biobert-v1.1'   # this can be changed to any model name used in the paper\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_name)\n",
    "LLM_model = AutoModel.from_pretrained(LLM_model_name)\n",
    "\n",
    "def encoding_sentences(sentences):    # function to encode sentences\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = LLM_model(**inputs)\n",
    "        # Mean pooling to get sentence embeddings\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "sentence_embeddings = encoding_sentences(data['abstract_lemmatized'])\n",
    "embedding_df = pd.DataFrame(sentence_embeddings)       #embeddings converted to a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we combines the metadata and journal features with the textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([data, tfidf_df], axis=  1)   # run this for TD_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([data, word2vec_df], axis=  1)           # run this for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([data, embedding_df], axis=  1)     # run this for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we split the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using just metatdata and journal features\n",
    "\n",
    "X = data.drop(columns = ['DOI', 'PMID', 'abstract_paper', 'target_variable', 'abstract+title', 'title_paper'])\n",
    "y = data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using just TF-IDF\n",
    "\n",
    "X = combined_data.drop(columns = ['DOI', 'PMID', 'abstract_paper', 'target_variable', 'abstract+title', 'title_paper'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using just word2vec\n",
    "\n",
    "X = combined_data.drop(columns = ['DOI', 'PMID', 'abstract_paper', 'target_variable', 'abstract+title', 'title_paper', 'word2vec_vector'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using just BERT features\n",
    "\n",
    "X = combined_data.drop(columns = ['DOI', 'PMID', 'abstract_paper', 'target_variable', 'abstract+title', 'title_paper'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for TF-IDF with metatdata and journal features\n",
    "\n",
    "X = combined_data.drop(columns = ['DOI', 'PMID', 'abstract_paper', 'target_variable', 'abstract+title', 'title_paper'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word2vec with metatdata and journal features\n",
    "\n",
    "X = combined_data.drop(columns = ['PMID', 'DOI', 'abstract_paper', 'target_variable', 'abstract_lemmatized', 'title_paper'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for BERT based models with metatdata and journal features\n",
    "\n",
    "X = combined_data.drop(columns = ['PMID', 'DOI', 'abstract_paper', 'target_variable', 'abstract_lemmatized', 'title_paper', 'abstract_tokenized'])\n",
    "y = combined_data.target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first for Gradient bossting classifier we find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [100, 200, 300],         # Number of trees in the forest\n",
    "    'learning_rate': [0.01, 0.1, 0.2],       # Learning rate\n",
    "    'min_samples_split': [2, 5, 10]         # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "\n",
    "model_gb = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=model_gb, param_grid=parameter_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "\n",
    "y_val_pred = best_gb.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "y_test_pred = best_gb.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we train and test on the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "model_gb_best = GradientBoostingClassifier(min_samples_split = 10, n_estimators = 300, learning_rate= 0.2)\n",
    "\n",
    "model_gb_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_gb_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "\n",
    "\n",
    "parameter_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],           # Inverse of regularization strength\n",
    "    'penalty': ['l2'],                      # Regularization type (L2 regularization)\n",
    "    'solver': ['liblinear', 'saga']         # Algorithms to use in the optimization problem\n",
    "}\n",
    "\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "\n",
    "grid_search_lr = GridSearchCV(estimator=model_lr, param_grid=parameter_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "\n",
    "y_val_pred = best_lr.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "y_test_pred = best_lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we train and test on the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "model_lr_best = LogisticRegression(max_iter=1000, C = 0.1, penalty='l2', solver='liblinear')\n",
    "\n",
    "model_lr_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_lr_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [100, 200, 300],         # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10]           # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=model_rf, param_grid=parameter_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we train and test on the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "model_rf_best = RandomForestClassifier(n_estimators=200, min_samples_split=2, max_depth=30)\n",
    "\n",
    "model_rf_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_rf_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "model_nb_best = GaussianNB()\n",
    "\n",
    "model_nb_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_nb_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "\n",
    "\n",
    "parameter_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]         # Function to measure the quality of a split\n",
    "}\n",
    "\n",
    "\n",
    "model_dt = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=model_dt, param_grid=parameter_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "\n",
    "\n",
    "y_val_pred = best_dt.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "y_test_pred = best_dt.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_dt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we train and test on the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "model_dt_best = DecisionTreeClassifier(min_samples_split=2, max_depth=None, min_samples_leaf=1)\n",
    "\n",
    "model_dt_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_dt_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
