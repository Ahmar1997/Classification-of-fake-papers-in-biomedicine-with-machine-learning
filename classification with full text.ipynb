{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows the classification of fake papers based on full text features \n",
    "\n",
    "Each cell has a description above it. If there are doubts in any parts of the code please contact at (ahmar.hussain@ovgu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with full text features i.e. readability scores, percentage of active voice, clause density, Type-Token Ratio etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "df_combined = pd.read_csv('D:\\\\new_dataset\\\\full text just grammar features.csv')\n",
    "\n",
    "X = df_combined.drop(columns = ['PMID', 'DOI', 'target_variable'])\n",
    "y = df_combined.target_variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "rf_mixed = GradientBoostingClassifier(min_samples_split = 10, n_estimators = 300, learning_rate= 0.2)\n",
    "\n",
    "rf_mixed.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_mixed.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarizing full text and classifying based on BERT embeddings of full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oreb45ap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\oreb45ap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings...\n",
      "Accuracy: 0.7367178276269185\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       639\n",
      "           1       0.46      0.38      0.41       208\n",
      "\n",
      "    accuracy                           0.74       847\n",
      "   macro avg       0.63      0.61      0.62       847\n",
      "weighted avg       0.72      0.74      0.73       847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "summarize_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "summarize_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "\n",
    "def text_extraction(path):\n",
    "    try:\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "        #extracting text after abstract uptill references\n",
    "\n",
    "        abstract = root.find('.//tei:abstract', namespaces=ns)\n",
    "        references = root.find('.//tei:div[@type=\"references\"]', namespaces=ns)\n",
    "        \n",
    "        if abstract is not None and references is not None:\n",
    "            abstract_index = list(root.iter()).index(abstract)\n",
    "            references_index = list(root.iter()).index(references)\n",
    "            extracted_text = []\n",
    "            for element in list(root.iter())[abstract_index:references_index]:\n",
    "                extracted_text.append(element.text or \"\")\n",
    "            return \" \".join(extracted_text)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Summarizing function\n",
    "def summarizing_text(text, max_length=150, min_length=50):\n",
    "    inputs = summarize_tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = summarize_model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summarized_text = summarize_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summarized_text\n",
    "\n",
    "# produce embeddings\n",
    "def embedding_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()  # Using [CLS] token\n",
    "    return embeddings.flatten()\n",
    "\n",
    "# Directories\n",
    "nonfakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\nonfakes\"    #These files can be found in the folder 'full text xmls' in the github repository\n",
    "fakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\fakes\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Text': [],\n",
    "    'Label': []  # 0 for non-fakes, 1 for fakes\n",
    "}\n",
    "\n",
    "\n",
    "for xml in os.listdir(nonfakes_directory):\n",
    "    if xml.endswith(\".xml\"):\n",
    "        path = os.path.join(nonfakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            # Summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(0)  # Non-fake\n",
    "\n",
    "\n",
    "for xml in os.listdir(fakes_directory):\n",
    "    if xml.endswith(\".xml\"):\n",
    "        path = os.path.join(fakes_directory, xml)\n",
    "        xml_text = text_extraction(path)\n",
    "        if xml_text is not None:\n",
    "            # Summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(1)  # Fake\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "dataframe['Label'] = encoder.fit_transform(dataframe['Label'])\n",
    "\n",
    "\n",
    "print(\"Generating BERT embeddings...\")\n",
    "embeddings = []\n",
    "for text in dataframe['Text']:\n",
    "    embeddings.append(embedding_text(text))\n",
    "\n",
    "\n",
    "embeddings_dataframe = pd.DataFrame(embeddings)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_dataframe, dataframe['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifying chunks and aggregating through majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oreb45ap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for each chunk...\n",
      "Document-level Accuracy: 0.7671084663240465\n",
      "Document-level Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85      2702\n",
      "           1       0.63      0.33      0.44       995\n",
      "\n",
      "    accuracy                           0.77      3697\n",
      "   macro avg       0.71      0.63      0.64      3697\n",
      "weighted avg       0.75      0.77      0.74      3697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def text_extraction(path):\n",
    "    try:\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "        #extracting text after abstract uptill references\n",
    "\n",
    "        abstract = root.find('.//tei:abstract', namespaces=ns)\n",
    "        references = root.find('.//tei:div[@type=\"references\"]', namespaces=ns)\n",
    "        \n",
    "        if abstract is not None and references is not None:\n",
    "            abstract_index = list(root.iter()).index(abstract)\n",
    "            references_index = list(root.iter()).index(references)\n",
    "            extracted_text = []\n",
    "            for element in list(root.iter())[abstract_index:references_index]:\n",
    "                extracted_text.append(element.text or \"\")\n",
    "            return \" \".join(extracted_text)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=False, padding=False, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    chunks = []\n",
    "    \n",
    "    # Split input IDs into chunks of max 512 tokens\n",
    "    for i in range(0, len(input_ids), max_length):\n",
    "        chunk = input_ids[i:i+max_length]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def embedding_chunk(chunk):\n",
    "    # Make sure `chunk` is in list format before creating tensor\n",
    "    inputs = {'input_ids': chunk.unsqueeze(0)}  # Convert to tensor\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()  # Use [CLS] token embedding\n",
    "    return embedding.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nonfakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\nonfakes\"        #These files can be found in the folder 'full text xmls' in the github repository\n",
    "fakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\fakes\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Text': [],\n",
    "    'Label': [],  # 0 for non-fakes, 1 for fakes\n",
    "    'Document_ID': []  \n",
    "}\n",
    "\n",
    "\n",
    "doc_id = 0\n",
    "\n",
    "for xml in os.listdir(nonfakes_directory):\n",
    "    if xml.endswith(\".xml\"):\n",
    "        path = os.path.join(nonfakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            # Summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(0)  # Non-fake\n",
    "            data['Document_ID'].append(doc_id)\n",
    "            doc_id += 1\n",
    "\n",
    "\n",
    "for xml in os.listdir(fakes_directory):\n",
    "    if xml.endswith(\".xml\"):\n",
    "        path = os.path.join(fakes_directory, xml)\n",
    "        xml_text = text_extraction(path)\n",
    "        if xml_text is not None:\n",
    "            # Summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(1)  # Fake\n",
    "            data['Document_ID'].append(doc_id)\n",
    "            doc_id += 1\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "dataframe['Label'] = encoder.fit_transform(df['Label'])\n",
    "\n",
    "\n",
    "chunk_embeddings = []\n",
    "chunk_labels = []\n",
    "chunk_document_ids = []\n",
    "\n",
    "print(\"Generating BERT embeddings for each chunk...\")\n",
    "for idx, row in dataframe.iterrows():\n",
    "    text = row['Text']\n",
    "    label = row['Label']\n",
    "    document_id = row['Document_ID']\n",
    "    \n",
    " \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "\n",
    "    for chunk in chunks:\n",
    "        embedding = embedding_chunk(chunk)\n",
    "        chunk_embeddings.append(embedding)\n",
    "        chunk_labels.append(label)\n",
    "        chunk_document_ids.append(document_id)\n",
    "\n",
    "\n",
    "chunk_embeddings_dataframe = pd.DataFrame(chunk_embeddings)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, doc_train, doc_test = train_test_split(\n",
    "    chunk_embeddings_dataframe, chunk_labels, chunk_document_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "def aggregating_predictions(chunk_predictions, ids):\n",
    "    predictions = {}\n",
    "    for doc_id in np.unique(ids):\n",
    "        # Get the predictions for all chunks of this document\n",
    "        doc_chunk_preds = [chunk_predictions[i] for i, d in enumerate(ids) if d == doc_id]\n",
    "        \n",
    "        # Aggregate via majority voting\n",
    "        most_common_pred = Counter(doc_chunk_preds).most_common(1)[0][0]\n",
    "        predictions[doc_id] = most_common_pred\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "doc_level_predictions = aggregating_predictions(y_pred, doc_test)\n",
    "\n",
    "\n",
    "doc_true_labels = {doc_id: y_test[i] for i, doc_id in enumerate(doc_test)}\n",
    "\n",
    "\n",
    "doc_ids_test = list(doc_true_labels.keys())\n",
    "y_true_doc = [doc_true_labels[doc_id] for doc_id in doc_ids_test]\n",
    "y_pred_doc = [doc_level_predictions[doc_id] for doc_id in doc_ids_test]\n",
    "\n",
    "doc_accuracy = accuracy_score(y_true_doc, y_pred_doc)\n",
    "\n",
    "print(f\"Document-level Accuracy: {doc_accuracy}\")\n",
    "print(\"Document-level Classification Report:\")\n",
    "print(classification_report(y_true_doc, y_pred_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
