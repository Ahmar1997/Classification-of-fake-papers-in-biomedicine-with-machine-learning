{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows the classification of fake papers based on full text features \n",
    "\n",
    "Each cell has a description above it. If there are doubts in any parts of the code please contact at (ahmar.hussain@ovgu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with full text features i.e. readability scores, percentage of active voice, clause density, Type-Token Ratio etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "df_combined = pd.read_csv('D:\\\\new_dataset\\\\full text just grammar features.csv')\n",
    "\n",
    "X = df_combined.drop(columns = ['PMID', 'DOI', 'target_variable'])\n",
    "y = df_combined.target_variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 2022,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "rf_mixed = GradientBoostingClassifier(min_samples_split = 10, n_estimators = 300, learning_rate= 0.2)\n",
    "\n",
    "rf_mixed.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_mixed.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarizing full text and classifying based on BERT embeddings of full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "BERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BERT_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "summarize_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "summarize_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "\n",
    "#summarizing function\n",
    "def summarizing_text(text, max = 150, min = 50):\n",
    "    inputs = summarize_tokenizer(text, return_tensors = 'pt', max_length = 1024, truncation = True)\n",
    "    summary_ids = summarize_model.generate(inputs['input_ids'], max_length = max, min_length = min, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summarized_text = summarize_tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "    return summarized_text\n",
    "\n",
    "#produce embeddings\n",
    "def embedding_text(text):\n",
    "    inputs = BERT_tokenizer(text, return_tensors = 'pt', max_length = 512, truncation = True, padding = 'max_length')\n",
    "    outputs = BERT_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach.numpy()\n",
    "    return embeddings.flatten()\n",
    "\n",
    "\n",
    "def text_extraction(xml_path):\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "        #extracting text after abstract uptill references\n",
    "\n",
    "        abstract = root.find('.//tei:abstract', namespaces=ns)\n",
    "        references = root.find('.//tei:div[@type=\"references\"]', namespaces=ns)\n",
    "\n",
    "        if abstract is not None and references is not None:\n",
    "            abstract_index = list(root.iter()).index(abstract)\n",
    "            reference_index = list(root.iter()).index(references)\n",
    "            text = []\n",
    "            for elem in list(root.iter())[abstract_index:reference_index]:\n",
    "                text.append(elem.text or \"\")\n",
    "            return \" \".join(text)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "# Directories\n",
    "nonfakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\nonfakes\"    #These files can be found in the folder 'full text xmls' in the github repository\n",
    "fakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\fakes\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Text': [],\n",
    "    'Label': []\n",
    "}\n",
    "\n",
    "\n",
    "#going through non fakes\n",
    "for xml in os.listdir(nonfakes_directory):\n",
    "    if xml.endswith('.xml'):\n",
    "        xml_path = os.path.join(nonfakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            #summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(0)\n",
    "\n",
    "\n",
    "#going through fakes\n",
    "for xml in os.listdir(fakes_directory):\n",
    "    if xml.endswith('.xml'):\n",
    "        xml_path = os.path.join(fakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            #summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(1)\n",
    "\n",
    "\n",
    "#converting to dataframe\n",
    "dataframe = pd.DataFrame(data)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "dataframe['Label'] = encoder.fit_transform(dataframe['Label'])\n",
    "\n",
    "embeddings = []\n",
    "for text in dataframe['Text']:\n",
    "    embeddings.append(embedding_text(text))\n",
    "\n",
    "\n",
    "embeddings_dataframe = pd.DataFrame(embeddings)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_dataframe, dataframe['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifying chunks and aggregating through majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oreb45ap\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for each chunk...\n",
      "Document-level Accuracy: 0.7671084663240465\n",
      "Document-level Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85      2702\n",
      "           1       0.63      0.33      0.44       995\n",
      "\n",
      "    accuracy                           0.77      3697\n",
      "   macro avg       0.71      0.63      0.64      3697\n",
      "weighted avg       0.75      0.77      0.74      3697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "BERT_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BERT_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def chunking_text(text, max = 512):\n",
    "    inputs = BERT_tokenizer(text, return_tensors = 'pt', max_length = max, truncation = False, padding=False, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(input_ids), max):\n",
    "        chunk = input_ids[i:i+max]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def embedding_chunks(chunk):\n",
    "    inputs = {'input_ids': chunk.unsqueeze(0)}\n",
    "    outputs = summarize_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach.numpy()\n",
    "    return embeddings.flatten()\n",
    "\n",
    "\n",
    "def text_extraction(xml_path):\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "        #extracting text after abstract uptill references\n",
    "\n",
    "        abstract = root.find(('.//tei:abstract', namespaces=ns))\n",
    "        references = root.find('.//tei:div[@type=\"references\"]', namespaces=ns)\n",
    "\n",
    "        if abstract is not None and references is not None:\n",
    "            abstract_index = list(root.iter()).index(abstract)\n",
    "            reference_index = list(root.iter()).index(references)\n",
    "            text = []\n",
    "            for elem in list(root.iter())[abstract_index:reference_index]:\n",
    "                text.append(elem.text or \"\")\n",
    "            return \" \".join(text)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "nonfakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\nonfakes\"        #These files can be found in the folder 'full text xmls' in the github repository\n",
    "fakes_directory = \"D:\\\\new_dataset\\\\full text xmls\\\\fakes\"\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Text': [],\n",
    "    'Label': [],  # 0 for non-fakes, 1 for fakes\n",
    "    'Document_ID': []  \n",
    "}\n",
    "\n",
    "doc_id = 0\n",
    "\n",
    "\n",
    "#going through non fakes\n",
    "for xml in os.listdir(nonfakes_directory):\n",
    "    if xml.endswith('.xml'):\n",
    "        xml_path = os.path.join(nonfakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            #summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(0)\n",
    "            data['Document_ID'].append(doc_id)\n",
    "            doc_id += 1\n",
    "\n",
    "\n",
    "#going through fakes\n",
    "for xml in os.listdir(fakes_directory):\n",
    "    if xml.endswith('.xml'):\n",
    "        xml_path = os.path.join(fakes_directory, xml)\n",
    "        xml_text = text_extraction(xml)\n",
    "        if xml_text is not None:\n",
    "            #summarize the extracted text\n",
    "            summarized_text = summarizing_text(xml_text)\n",
    "            data['Text'].append(summarized_text)\n",
    "            data['Label'].append(1)\n",
    "            data['Document_ID'].append(doc_id)\n",
    "            doc_id += 1\n",
    "\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "dataframe['Label'] = encoder.fit_transform(df['Label'])\n",
    "\n",
    "\n",
    "chunk_embeddings = []\n",
    "chunk_labels = []\n",
    "chunk_document_ids = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in dataframe.iterrows():\n",
    "    text = row['Text']\n",
    "    label = row['Label']\n",
    "    document_id = row['Document_ID']\n",
    "    \n",
    " \n",
    "    chunks = chunking_text(text)\n",
    "    \n",
    "\n",
    "    for chunk in chunks:\n",
    "        embedding = embedding_chunks(chunk)\n",
    "        chunk_embeddings.append(embedding)\n",
    "        chunk_labels.append(label)\n",
    "        chunk_document_ids.append(document_id)\n",
    "\n",
    "\n",
    "chunk_embeddings_dataframe = pd.DataFrame(chunk_embeddings)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, doc_train, doc_test = train_test_split(\n",
    "    chunk_embeddings_dataframe, chunk_labels, chunk_document_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "def aggregation(chunk_predictions, ids):\n",
    "    predictions = {}\n",
    "\n",
    "    for doc_id in np.unique(ids):\n",
    "        # Get the predictions for all chunks of this document\n",
    "        doc_chunk_preds = [chunk_predictions[i] for i, d in enumerate(ids) if d == doc_id]\n",
    "        \n",
    "        # Aggregate via majority voting\n",
    "        most_common_pred = Counter(doc_chunk_preds).most_common(1)[0][0]\n",
    "        predictions[doc_id] = most_common_pred\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "doc_level_predictions = aggregation(y_pred, doc_test)\n",
    "\n",
    "\n",
    "doc_true_labels = {doc_id: y_test[i] for i, doc_id in enumerate(doc_test)}\n",
    "\n",
    "\n",
    "doc_ids_test = list(doc_true_labels.keys())\n",
    "y_true_doc = [doc_true_labels[doc_id] for doc_id in doc_ids_test]\n",
    "y_pred_doc = [doc_level_predictions[doc_id] for doc_id in doc_ids_test]\n",
    "\n",
    "doc_accuracy = accuracy_score(y_true_doc, y_pred_doc)\n",
    "\n",
    "print(f\"Document-level Accuracy: {doc_accuracy}\")\n",
    "print(\"Document-level Classification Report:\")\n",
    "print(classification_report(y_true_doc, y_pred_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
